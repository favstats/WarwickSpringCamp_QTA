<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Quantitative Text Analysis in R</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <meta name="description" content="Quantitative Text Analysis in R"/>
    <meta name="generator" content="xaringan and remark.js"/>
    <meta name="github-repo" content="favstats/xxx"/>
    <meta name="twitter:title" content="Quantitative Text Analysis in R"/>
    <meta name="twitter:description" content="Quantitative Text Analysis in R"/>
    <meta name="twitter:url" content="https://www.favstats.eu"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:creator" content="@favstats"/>
    <meta name="twitter:site" content="@favstats"/>
    <meta property="og:title" content="Quantitative Text Analysis in R"/>
    <meta property="og:description" content="Quantitative Text Analysis in R"/>
    <meta property="og:url" content="https://www.favstats.eu"/>
    <meta property="og:type" content="website"/>
    <meta property="og:locale" content="en_US"/>
    <meta property="article:author" content="Fabio Votta"/>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/styles.css" type="text/css" />
    <link rel="stylesheet" href="css/fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">



layout: true

&lt;style&gt;
.onehundredtwenty {
  font-size: 120%;
   }

&lt;style&gt;
.ninety {
  font-size: 90%;
   }

.eightyfive {
  font-size: 85%;
   }
   
.eighty {
  font-size: 80%;
   }
   
.seventyfive {
  font-size: 75%;
   }
   
.seventy {
  font-size: 70%;
   }
   
.fifty {
  font-size: 50%;
   }
   
.forty {
  font-size: 40%;
   }
&lt;/style&gt;










---
name: title-slide
class: title-slide, center, middle


&lt;div class="my-logo-right"&gt;&lt;/div&gt; 

&lt;br&gt;

# .font150[.fancy[Quantitative Text Analysis in R]] 

### .font120[.fancy[A gentle hands-on introduction]]

*Spring Camp University of Warwick*

Instructor: Fabio Votta

[<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> @favstats](http://twitter.com/favstats)&lt;br&gt;
[<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> @favstats](http://github.com/favstats)&lt;br&gt;
[<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M336.5 160C322 70.7 287.8 8 248 8s-74 62.7-88.5 152h177zM152 256c0 22.2 1.2 43.5 3.3 64h185.3c2.1-20.5 3.3-41.8 3.3-64s-1.2-43.5-3.3-64H155.3c-2.1 20.5-3.3 41.8-3.3 64zm324.7-96c-28.6-67.9-86.5-120.4-158-141.6 24.4 33.8 41.2 84.7 50 141.6h108zM177.2 18.4C105.8 39.6 47.8 92.1 19.3 160h108c8.7-56.9 25.5-107.8 49.9-141.6zM487.4 192H372.7c2.1 21 3.3 42.5 3.3 64s-1.2 43-3.3 64h114.6c5.5-20.5 8.6-41.8 8.6-64s-3.1-43.5-8.5-64zM120 256c0-21.5 1.2-43 3.3-64H8.6C3.2 212.5 0 233.8 0 256s3.2 43.5 8.6 64h114.6c-2-21-3.2-42.5-3.2-64zm39.5 96c14.5 89.3 48.7 152 88.5 152s74-62.7 88.5-152h-177zm159.3 141.6c71.4-21.2 129.4-73.7 158-141.6h-108c-8.8 56.9-25.6 107.8-50 141.6zM19.3 352c28.6 67.9 86.5 120.4 158 141.6-24.4-33.8-41.2-84.7-50-141.6h-108z"/></svg> favstats.eu](https://www.favstats.eu)


28th June 2022 (Day 1)

.fifty[Link to slides: [favstats.github.io/WarwickSpringCamp_QTA/slides/day2/](https://favstats.github.io/WarwickSpringCamp_QTA/slides/day1/)]


---

### Your friendly neighborhood R Instructor

.leftcol40[
&lt;img src="images/me.jpg" style="width: 90%"/&gt;



]

.rightcol60[
+ Ph.D. Candidate in Political Communication at University of Amsterdam

+ Passionate about R and Data Science

+ I love to travel (when there is no pandemic)

+ I enjoy and (occasionally) create R memes

&lt;center&gt;

&lt;img src="https://pbs.twimg.com/profile_banners/866598352577867776/1495448334/1500x500" width="60%"&gt;

.font60[
[twitter.com/rstatsmemes](https://twitter.com/rstatsmemes)

&lt;/center&gt;

]



]

---

### It's not unusual to struggle at first but it gets better!

&lt;img src="https://raw.githubusercontent.com/favstats/ds3_r_intro/main/images/r_first_then_new.png" width="80%" style="display: block; margin: auto;" /&gt;


&lt;!-- ![](images/r_first_then_new.png){width=50%} --&gt;

.fifty[Illustration adapted from [Allison Horst](https://twitter.com/allison_horst)]

--

  + My experience is that this stuff isn't super easy... but it gets better!
  
--
  
+ Awesome inclusive community that is always ready to help
+ Active blogosphere with use cases and examples

---


class: center, middle, inverse

# .fancy[.red[Quantitative]] Text Analysis

  
---

class: center, middle, inverse

# .fancy[.red[Quantitative]] Text Analysis

&lt;center&gt;

&lt;img src="https://thumbs.gfycat.com/JaggedUglyGuillemot-max-1mb.gif"&gt;

  
&lt;/center&gt;
  
---


## .fancy[.red[Quantitative]] Text Analysis

.pull-left[
.font90[First, a note on *terms*:]

.font90[.red[Quantitative Text Analysis]] 

.font90[.blue[Computational Text Analysis]] 

.font90[.purple[Natural Language Processing (NLP)]] 

.font90[.orange[Text as Data] .green[Text Mining]]

.font90[**Here**, I will be using them interchangeably] 

.font50[(even though some peope might be mad at me for that)]

]

--

.pull-right[

![](https://media3.giphy.com/media/h19LzhEZ4VoNfS9SbK/giphy.gif)

]


---


## .fancy[.red[Quantitative]] vs. .fancy[.blue[Qualitative]] (?)

.pull-left[

.font80[
**Quantitative text analysis**

   + looks at **features of a text**
   
   + computers don't inherently understand what words or sentences are
   
   + transformation into metrics (word frequencies, co-occurrences etc.)
   
   + Goal: find/describe patterns and trends in text
   
   + "distant reading"
]



]


--

.pull-right[

.font80[

**Qualitative text analysis**

  + looks at the **meaning of a text** 
  
  + understand the author's point of view and its context
  
  + narrativize experiences of author/people mentioned in the text
  
  + Goal: interpret and understand viewpoints and concepts in text
  
  + "close reading"
  
]


  
]
   
   
--
  
&lt;br&gt;
  
In practice, these approaches will often overlap!

---



## .fancy[.red[Quantitative]] Text Analysis

**I N T E R D I S C I P L I N A R I T Y**

+ social sciences
+ digital humanities
+ data science
+ linguistics
+ media studies
+ computer science
+ ...

.font50[(at least in theory but too many times the institutional boundaries are too thick)]

---


## .fancy[.red[Quantitative]] Text Analysis

Four Principles of Automated Text Analysis (Grimmer &amp; Stewart 2013)

&gt; (1) All quantitative models of language are wrong—but some are useful.

&gt; (2) Quantitative methods for text amplify resources and augment humans.
 
&gt; (3) There is no globally best method for automated text analysis.

&gt; (4) Validate, Validate, Validate



---

&lt;img src="images/overview.png" style="width: 90%"/&gt;




---

&lt;img src="images/overview_drawn.png" style="width: 90%"/&gt;



---

### Overview Day 1 (13:00 - 16:00)


+ Introduction ✅

+ Manipulating strings in R 

+ Tokenization
  + Pre-processing (e.g. stop words removal)

15   M I N U T E S   B R E A K:   *14:30*

+ term frequency–inverse document frequency (tf-idf)

+ Stemming

+ Rule-based methods (i.e. dictionaries)

+ Some exercises

---


### Overview Day 2 (9:30 to 16:00)

+ Part-of-speech tagging
  + Keyword extraction
  + RAKE, Noun phrases, dependency parisng
  
15   M I N U T E S   B R E A K:   *10:45*

+ Unsupervised Machine Learning (Topic Modelling)
  
1   H O U R   B R E A K:   *12:00*

+ Supervised Machine Learning

15   M I N U T E S   B R E A K:   *14:30*

+ Project (45 minutes time)

*30 minutes for presentations and Q &amp; A*




---

class: center, middle, inverse

![](https://predictivehacks.com/wp-content/uploads/2020/11/tidyverse-default.png)

---

## What is the `tidyverse`?


.font80[

.pull-left[


The tidyverse describes itself:

&gt; The tidyverse is an opinionated **collection of R packages** designed for data science. All packages share an underlying design philosophy, grammar, and data structures.


```r
# install.packages("tidyverse")
library(tidyverse)
```
]



.pull-right[

All you need to learn about the tidyverse:


&lt;img src="https://d33wubrfki0l68.cloudfront.net/b88ef926a004b0fce72b2526b0b5c4413666a4cb/24a30/cover.png" width="40%"&gt;




Free book: [R for Data Science](https://r4ds.had.co.nz/)


]


]



---

## `tidyverse ` style syntax

.font80[

Using the pipe operator `%&gt;%` we can read code from left-to-right and from top-to-bottom.

This is opposed to base R functions that may have deeply nested function calls.

]

--


![](images/nomeme.png)

.font30[
Source: https://twitter.com/andrewheiss/status/1173743447171354624 (Andrew Heiss)
]



---


## `tidyverse ` style syntax

.font80[

Using the pipe operator `%&gt;%` we can read code from left-to-right and from top-to-bottom.

This is opposed to base R functions that may have deeply nested function calls.

]


![](https://pbs.twimg.com/media/EEqQqXsW4AAhtYz?format=jpg&amp;name=large)

---

## `tidyverse ` style syntax

Read from top to bottom and from left to right and the `%&gt;%` as *"and then"*.


```r
palmerpenguins::penguins %&gt;% 
  select(species, island, body_mass_g) %&gt;% 
  filter(island == "Dream") %&gt;% 
  mutate(bodymass_kg = body_mass_g/1000) %&gt;% 
  rename(which_type_of_pingu = species)
```

```
#&gt; # A tibble: 124 x 4
#&gt;    which_type_of_pingu island body_mass_g bodymass_kg
#&gt;    &lt;fct&gt;               &lt;fct&gt;        &lt;int&gt;       &lt;dbl&gt;
#&gt;  1 Adelie              Dream         3250        3.25
#&gt;  2 Adelie              Dream         3900        3.9 
#&gt;  3 Adelie              Dream         3300        3.3 
#&gt;  4 Adelie              Dream         3900        3.9 
#&gt;  5 Adelie              Dream         3325        3.32
#&gt;  6 Adelie              Dream         4150        4.15
#&gt;  7 Adelie              Dream         3950        3.95
#&gt;  8 Adelie              Dream         3550        3.55
#&gt;  9 Adelie              Dream         3300        3.3 
#&gt; 10 Adelie              Dream         4650        4.65
#&gt; # ... with 114 more rows
```

---


### `tidyverse ` functions reference list



.pull-left[


.font80[

For reference, here is a list of some useful  `tidyverse` functions.

If you have trouble with any of these functions, try reading the documentation with `?function_name` or ask me :)

]

.font40[

* `filter()`

  * Subset rows using column values

* `mutate()`

  * Create and modify delete columns

* `rename()`

  * Rename columns

* `select()`

  * Subset columns using their names and types

* `summarise()`; `summarize()`

  * Summarise each group to fewer rows

* `group_by()`; `ungroup()`

  * Group by one or more variables

* `arrange()`

  * Arrange rows by column values

]
]



.pull-right[



.font80[

&gt; Remember: tidyverse-style functions take the **data** as first argument.

&lt;br&gt;

]


.font40[

* `count()`; `tally()` 

  * Count observations by group

* `distinct()`

  * Subset distinct/unique rows

* `pull()`

  * Extract a single column

* `case_when()`

  * useful for recoding (when `ifelse` is not enough)

* `separate()`

  * separate two variables by some separator

* `pivot_wider()`

  * turn data into wide format

* `pivot_longer()`

  * turn data into long format

]


]




---

## Google Colab


For the purposes of this workshop we are going to use **Google Colab**.

Google Colab provides an online environment for teaching.

.blue[Upsides] 

+ No need to set up R and RStudio on your own computer

.red[Downsides]

+ You will have to set up R and RStudio on your own computer (when you want to use R on your own)


Link to the first Google Colab  (out of two): [tinyurl.com/wqtaday1](https://colab.research.google.com/drive/1cwaDldUvXMrtT1DT93UhhcdOBqATKM-E?usp=sharing)


---

class: center, middle, inverse

## Working with text in R


---

# *Strings* in R

--

.font80[


```r
"Hello I am a string! In R I am known by the class of 'character'"
```

```
#&gt; [1] "Hello I am a string! In R I am known by the class of 'character'"
```
]


--

.font80[

You can also have multiple texts in one vector by concatenating with `c()`


```r
c("Hello I am a string!",  "In R I am known by the class of 'character'")
```

```
#&gt; [1] "Hello I am a string!"                       
#&gt; [2] "In R I am known by the class of 'character'"
```
]

--


.font80[

And of course we may also assign this vector to an object using `&lt;-`


```r
chr_vector &lt;- c("Hello I am a string!",  "In R I am known by the class of 'character'")
chr_vector
```

```
#&gt; [1] "Hello I am a string!"                       
#&gt; [2] "In R I am known by the class of 'character'"
```
]

---

## *Strings* in data frames


```r
palmerpenguins::penguins_raw  %&gt;% 
  arrange(desc(`Culmen Length (mm)`)) %&gt;% head() %&gt;% 
  select(2:5) %&gt;% knitr::kable()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Sample Number &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Species &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Region &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Island &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 34 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gentoo penguin (Pygoscelis papua) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Biscoe &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Chinstrap penguin (Pygoscelis antarctica) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Dream &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 102 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gentoo penguin (Pygoscelis papua) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Biscoe &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 64 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Chinstrap penguin (Pygoscelis antarctica) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Dream &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gentoo penguin (Pygoscelis papua) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Biscoe &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 64 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gentoo penguin (Pygoscelis papua) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Anvers &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Biscoe &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---


class: center, middle

## Working with text in R

### String theory with the help of 

### `stringr`


```r
library(stringr)
```


---

.leftcol75[

### Manipulating Strings

.font80[
- `stringr` package is for manipulating strings. 
]

]

.rightcol25[


&lt;img src="https://stringr.tidyverse.org/logo.png" width="100" height="120" style="display: block; margin: auto 0 auto auto;" /&gt;

]

.font80[

- It has functions for 

  + **Detecting patterns in strings:** `str_detect()`
  
  + **replacing/removing parts:** `str_replace()` or `str_remove()`
  
  + **trimming whitespace:** `str_trim()` or `str_squish()`
  

]
  
--

| functions    | what it does                             |
|--------------|------------------------------------------|
| `str_count` | count how often a pattern occurs             |
| `str_extract`  | extract a pattern from a string           |
| `str_length`   | count how many characters does a string have        |
| `str_starts`, `str_ends` | tests if string starts/ends with certain pattern         |
| `str_to_lower`, `str_to_upper` | to lower/upper case |


---
  
### **Detecting**, extracting, replacing and removing patterns



Detect a pattern:


```r
str_detect(string = "Hello I am a string", pattern = "I am part of the string!")
```

```
#&gt; [1] FALSE
```

--


```r
str_detect(string = "Hello I am a string", pattern = "Hello")
```

```
#&gt; [1] TRUE
```
  
--


```r
str_detect(string = "Hello I am a string", pattern = "hello")
```

```
#&gt; [1] FALSE
```
  


---

### **Detecting**, extracting, replacing and removing patterns

Filter a dataset: **WITHOUT** `stringr`
  

```r
palmerpenguins::penguins_raw %&gt;% 
  filter(Species == "Gentoo penguin (Pygoscelis papua)")
```

```
#&gt; # A tibble: 124 x 17
#&gt;    studyName `Sample Number` Species         Region Island Stage `Individual ID`
#&gt;    &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          
#&gt;  1 PAL0708                 1 Gentoo penguin~ Anvers Biscoe Adul~ N31A1          
#&gt;  2 PAL0708                 2 Gentoo penguin~ Anvers Biscoe Adul~ N31A2          
#&gt;  3 PAL0708                 3 Gentoo penguin~ Anvers Biscoe Adul~ N32A1          
#&gt;  4 PAL0708                 4 Gentoo penguin~ Anvers Biscoe Adul~ N32A2          
#&gt;  5 PAL0708                 5 Gentoo penguin~ Anvers Biscoe Adul~ N33A1          
#&gt;  6 PAL0708                 6 Gentoo penguin~ Anvers Biscoe Adul~ N33A2          
#&gt;  7 PAL0708                 7 Gentoo penguin~ Anvers Biscoe Adul~ N34A1          
#&gt;  8 PAL0708                 8 Gentoo penguin~ Anvers Biscoe Adul~ N34A2          
#&gt;  9 PAL0708                 9 Gentoo penguin~ Anvers Biscoe Adul~ N35A1          
#&gt; 10 PAL0708                10 Gentoo penguin~ Anvers Biscoe Adul~ N35A2          
#&gt; # ... with 114 more rows, and 10 more variables: `Clutch Completion` &lt;chr&gt;,
#&gt; #   `Date Egg` &lt;date&gt;, `Culmen Length (mm)` &lt;dbl&gt;, `Culmen Depth (mm)` &lt;dbl&gt;,
#&gt; #   `Flipper Length (mm)` &lt;dbl&gt;, `Body Mass (g)` &lt;dbl&gt;, Sex &lt;chr&gt;,
#&gt; #   `Delta 15 N (o/oo)` &lt;dbl&gt;, `Delta 13 C (o/oo)` &lt;dbl&gt;, Comments &lt;chr&gt;
```
  
---
  
  
### **Detecting**, extracting, replacing and removing patterns
  
Filter a dataset: **WITH** `stringr`

  

```r
palmerpenguins::penguins_raw %&gt;% 
  filter(str_detect(Species, "Gentoo"))
```

```
#&gt; # A tibble: 124 x 17
#&gt;    studyName `Sample Number` Species         Region Island Stage `Individual ID`
#&gt;    &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          
#&gt;  1 PAL0708                 1 Gentoo penguin~ Anvers Biscoe Adul~ N31A1          
#&gt;  2 PAL0708                 2 Gentoo penguin~ Anvers Biscoe Adul~ N31A2          
#&gt;  3 PAL0708                 3 Gentoo penguin~ Anvers Biscoe Adul~ N32A1          
#&gt;  4 PAL0708                 4 Gentoo penguin~ Anvers Biscoe Adul~ N32A2          
#&gt;  5 PAL0708                 5 Gentoo penguin~ Anvers Biscoe Adul~ N33A1          
#&gt;  6 PAL0708                 6 Gentoo penguin~ Anvers Biscoe Adul~ N33A2          
#&gt;  7 PAL0708                 7 Gentoo penguin~ Anvers Biscoe Adul~ N34A1          
#&gt;  8 PAL0708                 8 Gentoo penguin~ Anvers Biscoe Adul~ N34A2          
#&gt;  9 PAL0708                 9 Gentoo penguin~ Anvers Biscoe Adul~ N35A1          
#&gt; 10 PAL0708                10 Gentoo penguin~ Anvers Biscoe Adul~ N35A2          
#&gt; # ... with 114 more rows, and 10 more variables: `Clutch Completion` &lt;chr&gt;,
#&gt; #   `Date Egg` &lt;date&gt;, `Culmen Length (mm)` &lt;dbl&gt;, `Culmen Depth (mm)` &lt;dbl&gt;,
#&gt; #   `Flipper Length (mm)` &lt;dbl&gt;, `Body Mass (g)` &lt;dbl&gt;, Sex &lt;chr&gt;,
#&gt; #   `Delta 15 N (o/oo)` &lt;dbl&gt;, `Delta 13 C (o/oo)` &lt;dbl&gt;, Comments &lt;chr&gt;
```


---
  
  
### Detecting, **extracting**, replacing and removing patterns
  
  
  

```r
str_extract(string = "Hello I am a string", pattern = "a")
```

```
#&gt; [1] "a"
```

```r
str_extract_all(string = "Hello I am a string", pattern = "a")
```

```
#&gt; [[1]]
#&gt; [1] "a" "a"
```
  
  
---
  
### Detecting, extracting, **replacing** and removing patterns
  
  
  

```r
str_replace(string = "Hello I am a string", pattern = "am", replacement = "REPLACED")
```

```
#&gt; [1] "Hello I REPLACED a string"
```



```r
str_replace_all(string = "Hello I am a string", pattern = " ", "---")
```

```
#&gt; [1] "Hello---I---am---a---string"
```
  
  
  
---
  
  
### Detecting, extracting, replacing and **removing** patterns
  
  
  

```r
str_remove(string = "Hello I am a string", pattern = "Hello ")
```

```
#&gt; [1] "I am a string"
```



```r
str_remove_all(string = "Hello I am a string", pattern = " ")
```

```
#&gt; [1] "HelloIamastring"
```


---



### Detecting, extracting, replacing and **removing** patterns



```r
str_trim(string = "        Hello I am a string with lots of whitespace around      ")
```

```
#&gt; [1] "Hello I am a string with lots of whitespace around"
```
  
  

```r
str_squish(string = "Hello    I am a     string with lots of whitespace inbetween")
```

```
#&gt; [1] "Hello I am a string with lots of whitespace inbetween"
```
  

---

class: center, middle

# .green[Reg]ular .blue[Ex]pression (.green[Reg].blue[ex])

.code60[`([_a-z0-9-]+(\\.[_a-z0-9-]+)\*@[a-z0-9-]+(\\.[a-z0-9-]+)\*(\\.[a-z]{2,4}))`]




---

## Regex

.code60[`([_a-z0-9-]+(\\.[_a-z0-9-]+)\*@[a-z0-9-]+(\\.[a-z0-9-]+)\*(\\.[a-z]{2,4}))`]

&gt; A regular expression (regex) is a special sequence of characters that describes a search pattern. 

--

.font90[The regex above can be used to extract E-Mail addresses from text:]


```r
some_text &lt;- c("Hi! This is my email: awesomemail@awesomewebsite.co.uk",
               "I have no E-mail but please have my number: +4918434342323")

str_extract(some_text, "([_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4}))")
```

```
#&gt; [1] "awesomemail@awesomewebsite.co.uk" NA
```



---

## Regex

.font90[Here is a non-exhaustive list of special regex characters that work specifically in R ([more](https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html)):]

| special R regex     | what it matches                                                                                             |
|-------------|-------------------------------------------------------------------------------------------------------------|
| `⁠[:lower:]⁠` | Lower-case letters in the current locale.                                                                   
| `⁠[:upper:]⁠` | Upper-case letters in the current locale.                                                                   |
| `⁠[:digit:]⁠` | Digits: `0 1 2 3 4 5 6 7 8 9⁠`.                                                                              |
| `⁠[:alpha:]⁠` | Alphabetic characters: `[:lower:]` and `⁠[:upper:]⁠`.                                                         |
| `⁠[:alnum:]⁠` | Alphanumeric characters: `⁠[:alpha:]` and `⁠[:digit:]⁠`.                                                       |
| `[:blank:]⁠` | Blank characters: space and tab, and possibly other locale-dependent characters such as non-breaking space. |
| `⁠[:punct:]⁠`| `⁠! " # $ % &amp; ' () \* + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ {  } ~⁠`                              |



---

## Regex

Remove punctuation:


```r
some_text &lt;- "Here are some numbers: 232323."

str_remove_all(some_text, "[:punct:]")
```

```
#&gt; [1] "Here are some numbers 232323"
```

Remove digits:


```r
str_remove_all(some_text, "[:digit:]")
```

```
#&gt; [1] "Here are some numbers: ."
```

Try some out yourself!⁠	

---


## Regex

Here are some more general regex special characters that typically work in most programming languages:

| symbols | what it matches                                                            |
|---------|----------------------------------------------------------------------------|
| `.⁠` |Placeholder character to match anything. |
| `⁠*`     | The preceding item will be matched **zero or more times**.                     |
| `⁠+⁠`     | The preceding item will be matched **one or more times**.                      |
| `{n}⁠`   | The preceding item is matched **exactly n times**.                             |
| `⁠{n,}⁠`  | The preceding item is matched **n or more times**.                             |
| `⁠{n,m}⁠` | The preceding item is matched **at least n times, but not more than m times**. |

---

## Regex


Consider the following sequence of strings that are variations of "Francis Fukuyama" but some of them are misspelled. We can come up with a regex that takes take care of all variations:


```r
some_text &lt;- c("Francis Fukuyama", "francis fukuyama", "franis fukuyama", "franiss fukuyama")

str_extract(some_text, ".ranc*is{1,2} .ukuyama")
```

```
#&gt; [1] "Francis Fukuyama" "francis fukuyama" "franis fukuyama"  "franiss fukuyama"
```

Some more resources to learn about regex:

1. https://stringr.tidyverse.org/articles/regular-expressions.html
2. https://spannbaueradam.shinyapps.io/r_regex_tester/

---

#### Small Regexercise(s)

.font70[Find the appropriate regex to extract each (mis)spelling of Nicolas Cage's name]

![](https://github.com/favstats/WarwickSpringCamp_QTA/blob/main/docs/slides/day1/images/nicolas.png?raw=true)

.panelset[
.panel[.panel-name[Strings]



```r
cage_text &lt;- c("Nicolas Cage", "Nicolas Rage", "Nick Cage", "Nicolás Cagé", "Saint-Nicolas Cage")
```

]


.panel[.panel-name[Solution]


```r
str_extract(cage_text, ".{0,6}Nic.{1,4} .ag.")
```

```
#&gt; [1] "Nicolas Cage"       "Nicolas Rage"       "Nick Cage"         
#&gt; [4] "Nicolás Cagé"       "Saint-Nicolas Cage"
```

]

]

---

#### Small Regexercise(s)

.font70[Find the appropriate regex to extract each (mis)spelling of Mark Wahlberg's name]

![](https://github.com/favstats/WarwickSpringCamp_QTA/blob/main/docs/slides/day1/images/markimoo.png?raw=true)

.panelset[
.panel[.panel-name[Strings]



```r
markimoo_text &lt;- c("Mark Wahlberg", "Marc waalberg", "Mahrk Vahlbehrk", "Mark Wahlbergensis", "Mark the Man")
```

]


.panel[.panel-name[Solution]


```r
str_extract(markimoo_text, "Ma.*r. .a.lbe.*r.{1,6}|Mark the Man")
```

```
#&gt; [1] "Mark Wahlberg"      "Marc waalberg"      "Mahrk Vahlbehrk"   
#&gt; [4] "Mark Wahlbergensis" "Mark the Man"
```

]
]

---

class: center, middle

#  Pre-processing

&lt;center&gt;

&lt;img src="https://cdn.dribbble.com/users/648258/screenshots/2502894/dribbble.gif" width = "55%"&gt;

&lt;/center&gt;

.font30[Source: https://cdn.dribbble.com/users/648258/screenshots/2502894/dribbble.gif]



---

## Pre-processing

.pull-left[

.font90[
Why pre-process?

1. Remove (replace) errors in the data
  + errors can happen when you scrape data
  + html tags (`&lt;i&gt;Hello&lt;i/&gt;`), encoding errors (`???☐☐☐`), etc.
2. Remove "uninteresting" text
  + misspellings?, URLs? words like "the", "and"?
  + *caution*: what is uninteresting?
  + depending on the context they might be very important!

]


]

.pull-right[

.font90[
&gt; Pre-processing is whatever steps you (need to) take to clean up your data before you start your analysis.


]



]




---

## Pre-processing

.pull-left[

.font90[
Why pre-process?

1. Remove (replace) errors in the data
  + errors can happen when you scrape data
  + html tags (`&lt;i&gt;Hello&lt;i/&gt;`), encoding errors (`???☐☐☐`), etc.
2. Remove "uninteresting" text
  + misspellings?, URLs? words like "the", "and"?
  + *caution*: what is uninteresting?
  + depending on the context they might be very important!

]

]

.pull-right[

.font90[
&gt; Pre-processing is whatever steps you (need to) take to clean up your data before you start your analysis.

&gt; Always think what measures of pre-processing are appropriate for your data! 

Doe is it help understand the data? Introduce more bias?

]



]


---

## Pre-processing




.pull-left[

.font90[
Why pre-process?

1. Remove (replace) errors in the data
  + errors can happen when you scrape data
  + html tags (`&lt;i&gt;Hello&lt;i/&gt;`), encoding errors (`???☐☐☐`), etc.
2. Remove "uninteresting" text
  + misspellings?, URLs? words like "the", "and"?
  + *caution*: what is uninteresting?
  + depending on the context they might be very important!

]

]

.pull-right[

.font90[
&gt; Pre-processing is whatever steps you (need to) take to clean up your data before you start your analysis.

&gt; Always think what measures of pre-processing are appropriate for your data! 

&gt; **EITHER WAY:** Always document your pre-processing approach so others can judge and reproduce the steps you take.

]



]

---

## Pre-processing

**Common methods of pre-processing**

1. Removing numbers
2. Removing punctuation
3. Removing whitespace
4. Text to lower case
5. Remove stop words
6. Stemming/Lemmatization

---


## Pre-processing

**Common methods of pre-processing**

1. Removing numbers `str_remove_all(text, "[:digit:]")`
2. Removing punctuation `str_remove_all(text, "[:punct:]")`
3. Removing whitespace `str_trim(text)` or `str_squish(text)`
4. Text to lower case `str_to_lower(text)`
5. Remove stop words
6. Stemming/Lemmatization

We learned how to do most of these already.

We are going to talk more about *removing stop words* and *stemming/lemmatization*. 

But first we need to introduce `tokenization`.

---

class: center, middle

# Tokenization

&lt;center&gt;

&lt;img src="https://animesher.com/orig/1/149/1492/14923/animesher.com_food-gif-anime-food-anime-gif-1492308.gif" width = "70%"&gt;

&lt;/center&gt;

---


## Tokenization


&gt; Tokenization is the process of breaking down a string of text into smaller elements called tokens. 

&gt; Tokens can be individual words, phrases, or even complete sentences. 

Why tokenize?

Breaking text into smaller (meaningful) can help create metrics from patterns that the computer can understand

(more about that later)

---

## Tokenization


**Example of *white-space tokenization*:**

`I`  `am`  `a`   `sentence`

&amp;#8595; T O K E N I Z E &amp;#8595;

| `I` | `am` | `a` | `sentence` |


This will give us words (in most cases)


---


## Tokenization

**Example of *dot tokenization*:**

`I am a sentence.` `Here is the second one.`


&amp;#8595; T O K E N I Z E &amp;#8595;


| `I am a sentence` | `Here is the second one`


This will give us words (in many cases), but certainly not foolproof!

In reality, good tokenizers need to do more than just split a string by white-space or dots.

---

class: center, middle

## How can we tokenize in R?

We will do this shorty but first:

A short excourse in .fancy[tidy data] principles

---

## Short excourse: tidy data

.pull-left[


Definition of tidy data:

* Every column is a variable.
* Every row is an observation.
* Every cell is a single value.

]

.pull-right[

Here is an example of tidy data:

| animal | weight |  friendliness  |
| --- | --- | --- | 
| Domestic dog | 24.0 | 5.00 |
| Domestic cat | 12.0 | 3.08 |
| American alligator | 77.0 | 1.00 | 
| Golden hamster | 3.9 | 5.00 |
| King penguin | 26.0 |  3.71 |

]



---

## Untidy data

| Animal | weight/friendliness  |
| --- | --- | 
| Domestic dog | 24.0 / 5.00 |
| Domestic cat | 12.0 / 3.08 |
| American alligator | 77.0 / 1.00 | 
| Golden hamster | 3.9 / 5.00 |
| King penguin | 26.0 /  3.71 |

The data above has multiple variables per column.

= not tidy

---

## Core principle: tidy data

&lt;center&gt;
&lt;img src="https://www.openscapes.org/img/blog/tidydata/tidydata_2.jpg" style="width: 80%" /&gt;
&lt;/center&gt;

.fifty[Artist: [Allison Horst](https://github.com/allisonhorst)]
---

## Core principle: tidy data

Tidy data has two decisive advantages:

* Consistently prepared data is easier to read, process, load and save.

* Many procedures (or the associated functions) in R require this type of data.

&lt;center&gt;
&lt;img src="https://www.openscapes.org/img/blog/tidydata/tidydata_4.jpg" style="width: 40%" /&gt;
&lt;/center&gt;

.fifty[Artist: [Allison Horst](https://github.com/allisonhorst)]

---

## Tidy text data

.pull-left[

&gt; "We thus define the tidy text format as being a table with **one-token-per-row**. *~Text Mining with R (Julia Silge and David Robinson)*

The **one-token-per-row** can be a single word, but can also an multiple words (n-gram), sentence, or paragraph.

]

.pull-right[

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; token_id &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; token &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; upos &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Hurricane &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; NOUN &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; on &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ADP &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; its &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; PRON &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; way &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; NOUN &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; to &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ADP &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; the &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; DET &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Florida &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; PROPN &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Pan &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; PROPN &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Handle &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; PROPN &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]





---

.leftcol75[

## Text mining with `tidytext`

.font80[
The `tidytext` package provides various functions and support for manipulating text adhering to tidy data principles.
]

]

.rightcol25[


&lt;img src="https://juliasilge.github.io/tidytext/reference/figures/logo.png" width="100" height="120" style="display: block; margin: auto 0 auto auto;" /&gt;

]


  
.pull-left[

+ Pre-processing (stop words removal)

+ Tokenization

+ tf-idf (**t**erm **f**requency-**i**nverse **d**ocument **f**requency)

Free book hosted online: [tidytextmining.com](https://www.tidytextmining.com/)

]

.pull-right[
&lt;img src="https://www.tidytextmining.com/images/cover.png" width = "55%"&gt;

]

---


class: center, middle

## How can we tokenize in R?

.fancy[this time for realsies]

---

## Tokenization

To make these examples more fun we will use some Trump tweets to perform tokenization.


```r
## read in Trump tweets
trump_tweets &lt;- readr::read_csv("https://raw.githubusercontent.com/favstats/WarwickSpringCamp_QTA/main/docs/slides/day1/data/trump_tweets.csv")
```


```r
## nuclear tweet
trump_tweet &lt;- trump_tweets[trump_tweets$id == 1165918301932916736,]
```


```r
trump_tweet %&gt;%
  pull(text)
```

```
#&gt; [1] "The story by Axios that President Trump wanted to blow up large hurricanes with nuclear weapons prior to reaching shore is ridiculous. I never said this. Just more FAKE NEWS!"
```


---

### tokenizing with `unnest_tokens()`

`unnest_tokens` is the main workhorse of `tidytext` with it we can tokenize as words, sentences and more! 

It has three main arguments:

1. `output`: what should the new tokenized variable be called
2. `input`: what is the text variable that includes the text (already present in the dataset)
3. `token`: What are your tokens gonna be? Can be `"words" (default)`, `"characters"`, `"character_shingles"`, `"ngrams"`, `"skip_ngrams"`, `"sentences"`, `"lines"`, `"paragraphs"`, `"regex"`, `"tweets"` (tokenization by word that preserves usernames, hashtags, and URLS)

`unnest_tokens(text_data, output = word, input = text, token = "words")`

---

### tokenizing with `unnest_tokens()`


```r
library(tidytext)

trump_tweet %&gt;%
  unnest_tokens(output = word, input = text, token = "words") %&gt;%
  pull(word)
```

```
#&gt;  [1] "the"        "story"      "by"         "axios"      "that"      
#&gt;  [6] "president"  "trump"      "wanted"     "to"         "blow"      
#&gt; [11] "up"         "large"      "hurricanes" "with"       "nuclear"   
#&gt; [16] "weapons"    "prior"      "to"         "reaching"   "shore"     
#&gt; [21] "is"         "ridiculous" "i"          "never"      "said"      
#&gt; [26] "this"       "just"       "more"       "fake"       "news"
```

---


### tokenizing with `unnest_tokens()`

Keep uppercase letters and punctuation:


```r
trump_tweet %&gt;%
  unnest_tokens(output = word, input = text, token = "words", 
                to_lower = FALSE, strip_punct = FALSE) %&gt;%
  pull(word)
```

```
#&gt;  [1] "The"        "story"      "by"         "Axios"      "that"      
#&gt;  [6] "President"  "Trump"      "wanted"     "to"         "blow"      
#&gt; [11] "up"         "large"      "hurricanes" "with"       "nuclear"   
#&gt; [16] "weapons"    "prior"      "to"         "reaching"   "shore"     
#&gt; [21] "is"         "ridiculous" "."          "I"          "never"     
#&gt; [26] "said"       "this"       "."          "Just"       "more"      
#&gt; [31] "FAKE"       "NEWS"       "!"
```

---


### tokenizing with `unnest_tokens()`

We can also easily create sentences as tokens:


```r
trump_tweet %&gt;%
  unnest_tokens(output = word, input = text, token = "sentences") %&gt;%
  pull(word)
```

```
#&gt; [1] "the story by axios that president trump wanted to blow up large hurricanes with nuclear weapons prior to reaching shore is ridiculous."
#&gt; [2] "i never said this."                                                                                                                    
#&gt; [3] "just more fake news!"
```

---


### tokenizing with `unnest_tokens()`

We can also create so-called **ngrams**, words that follow each other:

**bigrams**: two words that follow each other


```r
trump_tweet %&gt;%
  unnest_tokens(output = bigrams, input = text, token = "ngrams", n = 2) %&gt;%
  pull(bigrams)
```

```
#&gt;  [1] "the story"        "story by"         "by axios"         "axios that"      
#&gt;  [5] "that president"   "president trump"  "trump wanted"     "wanted to"       
#&gt;  [9] "to blow"          "blow up"          "up large"         "large hurricanes"
#&gt; [13] "hurricanes with"  "with nuclear"     "nuclear weapons"  "weapons prior"   
#&gt; [17] "prior to"         "to reaching"      "reaching shore"   "shore is"        
#&gt; [21] "is ridiculous"    "ridiculous i"     "i never"          "never said"      
#&gt; [25] "said this"        "this just"        "just more"        "more fake"       
#&gt; [29] "fake news"
```

---



### tokenizing with `unnest_tokens()`



**trigrams**: three words that follow each other


```r
trump_tweet %&gt;%
  unnest_tokens(output = trigrams, input = text, token = "ngrams", n = 3) %&gt;%
  pull(trigrams)
```

```
#&gt;  [1] "the story by"            "story by axios"         
#&gt;  [3] "by axios that"           "axios that president"   
#&gt;  [5] "that president trump"    "president trump wanted" 
#&gt;  [7] "trump wanted to"         "wanted to blow"         
#&gt;  [9] "to blow up"              "blow up large"          
#&gt; [11] "up large hurricanes"     "large hurricanes with"  
#&gt; [13] "hurricanes with nuclear" "with nuclear weapons"   
#&gt; [15] "nuclear weapons prior"   "weapons prior to"       
#&gt; [17] "prior to reaching"       "to reaching shore"      
#&gt; [19] "reaching shore is"       "shore is ridiculous"    
#&gt; [21] "is ridiculous i"         "ridiculous i never"     
#&gt; [23] "i never said"            "never said this"        
#&gt; [25] "said this just"          "this just more"         
#&gt; [27] "just more fake"          "more fake news"
```

---

class: center, middle

## Word Frequencies

Let's count the words most often used by the former US President:

&lt;center&gt;

&lt;img src="https://c.tenor.com/HTNcED86kS4AAAAM/debate-debate2016.gif"/&gt;

&lt;/center&gt;


---

### Word Frequencies


.font80[First, we tokenize all tweets into words:]

.code60[

```r
trump_words &lt;- trump_tweets %&gt;%
  unnest_tokens(word, text, token = "words")
```
]



.font80[Then, we simply count the number of words:]


.code60[

```r
trump_words %&gt;% 
  count(word, sort = T) %&gt;%
  slice(1:20) %&gt;% 
  mutate(word_n = paste0(word, ": ", n)) %&gt;% 
  pull(word_n)
```

```
#&gt;  [1] "the: 46030"             "to: 26311"              "and: 21157"            
#&gt;  [4] "a: 19293"               "of: 18024"              "t.co: 16669"           
#&gt;  [7] "is: 16202"              "in: 15961"              "for: 12845"            
#&gt; [10] "https: 12204"           "you: 11607"             "i: 10979"              
#&gt; [13] "realdonaldtrump: 10945" "rt: 10167"              "on: 10156"             
#&gt; [16] "will: 8313"             "be: 8202"               "great: 7672"           
#&gt; [19] "that: 7565"             "are: 7418"
```
]


---

## Pre-processing: stop words


```
#&gt;  [1] "the: 46030"             "to: 26311"              "and: 21157"            
#&gt;  [4] "a: 19293"               "of: 18024"              "t.co: 16669"           
#&gt;  [7] "is: 16202"              "in: 15961"              "for: 12845"            
#&gt; [10] "https: 12204"           "you: 11607"             "i: 10979"              
#&gt; [13] "realdonaldtrump: 10945" "rt: 10167"              "on: 10156"             
#&gt; [16] "will: 8313"             "be: 8202"               "great: 7672"           
#&gt; [19] "that: 7565"             "are: 7418"
```


Ok. So there are a lot of words that are not that informative on their own like: `the`, `a`, `you`, etc. These words are commonly referred to as **stop words**.

&gt; stop words are words that are commonly used in a language but may have little meaning on their own. For bag-of-words approaches (i.e. looking at individual words without context) they are often removed from text in pre-processing because they can may hide more meaningful words. 



---

### Pre-processing: stop words

.font85[
&gt; **However**, removing stop words is a very crude method and you should inspect the list of stop words *carefully* for your use-case before you remove anything.
]

.font85[`tidytext` comes with several lexicons of stop words, some of them have more or less words defined as stop words:]


.panelset[
.panel[.panel-name[Show lexicons]



```r
stop_words %&gt;% count(lexicon)
```

```
#&gt; # A tibble: 3 x 2
#&gt;   lexicon      n
#&gt;   &lt;chr&gt;    &lt;int&gt;
#&gt; 1 onix       404
#&gt; 2 SMART      571
#&gt; 3 snowball   174
```
]


.panel[.panel-name[Some stop words]

```r
sample(stop_words$word, 10)
```

```
#&gt;  [1] "new"       "evenly"    "you'll"    "wasn't"    "it'll"     "specify"  
#&gt;  [7] "certain"   "downwards" "had"       "anyhow"
```
]

]

---

### Pre-processing: stop words



.font80[For the sake of simplicity, let's just remove stop words from all provided lexicons to see if we can find some patterns in what Trump was talking about. *Note: this would normally not be advised without thoroughly checking the stop words yourself.*]

.panelset[
.panel[.panel-name[R Code]


```r
trump_words %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = T)  %&gt;%
  slice(1:20) %&gt;% 
  mutate(word_n = paste0(word, ": ", n)) %&gt;% 
  pull(word_n)
```
]

.panel[.panel-name[Output]


```
#&gt;  [1] "t.co: 16669"            "https: 12204"           "realdonaldtrump: 10945"
#&gt;  [4] "rt: 10167"              "trump: 6573"            "amp: 5682"             
#&gt;  [7] "president: 4699"        "http: 4550"             "people: 3497"          
#&gt; [10] "country: 2338"          "america: 2258"          "time: 2048"            
#&gt; [13] "donald: 1921"           "news: 1860"             "democrats: 1826"       
#&gt; [16] "obama: 1752"            "vote: 1746"             "american: 1481"        
#&gt; [19] "job: 1300"              "fake: 1297"
```
]
]






---

### Pre-processing: stop words


Looks like there are still a bunch of words (mostly Twitter artifacts like url fragments, user names) that are not so meaningful. 

We can add more stop words to the existing list of stop words like this:

.panelset[
.panel[.panel-name[Create new stop words and add to old one]


```r
additional_stop_words &lt;- data.frame(word = c("t.co", "https", "realdonaldtrump", "rt", "amp", "http"))

enhanced_stop_words &lt;- stop_words %&gt;%
  bind_rows(additional_stop_words)
```


]

.panel[.panel-name[Remove new stop words and count]


```r
trump_words %&gt;%
  anti_join(enhanced_stop_words) %&gt;%
  count(word, sort = T)  %&gt;%
  slice(1:20) %&gt;% 
  mutate(word_n = paste0(word, ": ", n)) %&gt;% 
  pull(word_n)
```
]

.panel[.panel-name[Output]


```
#&gt;  [1] "trump: 6573"     "president: 4699" "people: 3497"    "country: 2338"  
#&gt;  [5] "america: 2258"   "time: 2048"      "donald: 1921"    "news: 1860"     
#&gt;  [9] "democrats: 1826" "obama: 1752"     "vote: 1746"      "american: 1481" 
#&gt; [13] "job: 1300"       "fake: 1297"      "run: 1283"       "u.s: 1277"      
#&gt; [17] "election: 1215"  "love: 1191"      "media: 1188"     "win: 1180"
```
]
]

---

## What are the top words used by Trump by year?

.panelset[
.panel[.panel-name[Count top words per year]

```r
top10_words_per_year &lt;- trump_words  %&gt;%
  anti_join(enhanced_stop_words) %&gt;%
  count(date_year, word, sort = T) %&gt;%
  arrange(-n) %&gt;%
  filter(date_year %in% 2016:2021)  %&gt;%
  group_by(date_year) %&gt;%
  slice(1:10) %&gt;%
  ungroup()
```
]
.panel[.panel-name[R code for visualization]

```r
library(ggplot2)

top10_words_per_year %&gt;% 
  mutate(word = reorder_within(word, n, date_year)) %&gt;%
  ggplot(aes(word, n, fill = date_year)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~date_year, scales = "free") +
  scale_x_reordered() +
  coord_flip()
```
]

.panel[.panel-name[Visualization]
&lt;img src="figs/unnamed-chunk-53-1.png" width="936" /&gt;
]
]









---


class: center, middle, inverse

# tf-idf

&lt;center&gt;

&lt;img src="https://media3.giphy.com/media/3otPoHK9wPQ9wCmrM4/giphy.gif" style="width: 70%"/&gt;

&lt;/center&gt;

---

## Some terminology


+ **Document**

  + In quantitative text analysis, when we talk about documents, we mean some broader unit of text, for example a speech, or a chapter in a book.

+ **Corpus**

  + A collection of documents

.rightcol30[
![](https://c.tenor.com/EcYe7AAKmikAAAAM/filing-shall-i-file-it.gif)
]


  
---

## Small exercise

We have three documents. Each includes a statement:

| Document 1                                  | Document 2                                         | Document 3                                                      |
|---------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------|
| My cat is too much good. The world is a great place.  | The planet is a terrible place. The world comes with too much suffering. | The place is terrible, with a bad WiFi connection, too. |

Imagine you are explaining to someone what each document is about using the words from that document. 

1. Which words are the least informative and why?

2. Which words are the most informative and why?

---


## Small exercise

We have three documents. Each includes a statement:

| Document 1                                  | Document 2                                         | Document 3                                                      |
|---------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------|
| My cat **.red[is]** **.red[too]** much good. **.red[The]** world is **.red[a]** great **.red[place]**.  | **.red[The]** planet **.red[is]** **.red[a]** terrible **.red[place]**. **.red[The]** world comes with **.red[too]** much suffering. | **.red[The]** **.red[place]** **.red[is]** terrible, with **.red[a]** bad WiFi connection, **.red[too]**. |


&gt; Answer 1: The words that are least informative about the documents are the ones that occur in all three: `place`, `"the"`, `"is"`, `"too"`, `a`.



---

## Small exercise

We have three documents. Each includes a statement:

| Document 1                                  | Document 2                                         | Document 3                                                      |
|---------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------|
| **.blue[My]** **.blue[cat]** is too much **.blue[good]**. The world is a **.blue[great]** place.  | The **.blue[planet]** is a terrible place. The world comes with too much **.blue[suffering]**. | The place is terrible, comes with a **.blue[bad WiFi connection]**, too. |



&gt; Answer 2: The words that are most informative about the documents are the ones that uniquely (or more often) appear in one document over the others. Document 1: `my`, `cat`, `good`, `great`. Document 2: `planet`. `suffering`. Document 3: `bad`, `wifi`, `connection`.

---

## Quantifying *informativeness* with tf-idf

What is tf-idf?

&gt; Tf-idf is a technique used for ranking terms based on the frequency of a given term in a range of documents. Tf-idf stands for **t**erm **f**requency-**i**nverse **d**ocument **f**requency. 

.font85[

TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).

IDF(t) = log(Total number of documents / Number of documents with term t in it).

]




.font70[
The term frequency (tf) is the number of times a given term appears in a document. The inverse document frequency (idf) is the number of documents that contain the given term divided by the total number of documents. The tf-idf score is the product of the term frequency and inverse document frequency.
]


---

## Quantifying *informativeness* with tf-idf

.font80[
| Document 1                                  | Document 2                                         | Document 3                                                      |
|---------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------|
| My cat **is** too much good. The world **is** a great place.  | The planet **is** a terrible place. The world comes with too much suffering. | The place **is** terrible, with a bad WiFi connection, too. |
]



.panelset[
.panel[.panel-name[Calculate tf]

.font70[TF(`is`) = (Number of times term **is** appears in a document) / (Total number of terms in the document).]


```r
freq_is &lt;- c(doc1 = 2, doc2 = 1, doc3 = 1)
n_terms &lt;- c(doc1 = 12, doc2 = 13, doc3 = 11)

tf &lt;- freq_is/n_terms

tf
```

```
#&gt;       doc1       doc2       doc3 
#&gt; 0.16666667 0.07692308 0.09090909
```

]



.panel[.panel-name[Calculate idf]

.font70[IDF(`is`) = log(Total number of documents / Number of documents with term **is** in it).]


```r
n_doc &lt;- 3
n_doc_is &lt;- 3

idf &lt;- log(n_doc / n_doc_is)

idf
```

```
#&gt; [1] 0
```

]


.panel[.panel-name[Calculate tf-idf]


```r
tf_idf &lt;- tf*idf
tf_idf
```

```
#&gt; doc1 doc2 doc3 
#&gt;    0    0    0
```

&gt; It's 0, which means this term is very uninformative for any of the documents! 

]

]




---

## Quantifying *informativeness* with tf-idf

.font80[
| Document 1                                  | Document 2                                         | Document 3                                                      |
|---------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------|
| My cat is too much good. The world is a great place.  | The planet is a **terrible** place. The world comes with too much suffering. | The place is **terrible**, with a bad WiFi connection, too. |
]



.panelset[
.panel[.panel-name[Calculate tf]

.font70[TF(`terrible`) = (Number of times **terrible** appears in document) / (number of terms in the document).]


```r
freq_is &lt;- c(doc1 = 0, doc2 = 1, doc3 = 1)
n_terms &lt;- c(doc1 = 12, doc2 = 13, doc3 = 11)

tf &lt;- freq_is/n_terms

tf
```

```
#&gt;       doc1       doc2       doc3 
#&gt; 0.00000000 0.07692308 0.09090909
```

]



.panel[.panel-name[Calculate idf]

.font70[IDF(`terrible`) = log(Total number of documents / Number of documents with term **terrible** in it).]


```r
n_doc &lt;- 3
n_doc_terrible &lt;- 2

idf &lt;- log(n_doc / n_doc_terrible)

idf
```

```
#&gt; [1] 0.4054651
```

]


.panel[.panel-name[Calculate tf-idf]


```r
tf_idf &lt;- tf*idf
tf_idf
```

```
#&gt;       doc1       doc2       doc3 
#&gt; 0.00000000 0.03118962 0.03686046
```

&gt; The term `terrible` is very uninformative for document 1, but more important for document 2 and 3! 


]

]


---

## Calculate tf-idf with `bind_tf_idf()`

Using the `bind_tf_idf` (from `tidytext`) we can calculate tf-idf scores in R. It takes as input a data frame with token frequencies by document.

.panelset[

.panel[.panel-name[Create documents]


```r
the_docs &lt;- data.frame(doc_id = paste0("Document ",1:3), text = c("My cat is too much good. The world is a great place.", "The planet is a terrible place. The world comes with too much suffering.", "The place is terrible, comes with a bad WiFi connection, too."))
```

]

.panel[.panel-name[Get tf-idf]



```r
docs_tf_idf &lt;- the_docs %&gt;%
  unnest_tokens(word, text) %&gt;% 
  count(doc_id, word, sort = T) %&gt;%
  bind_tf_idf(word, doc_id, n)
```


]



.panel[.panel-name[Visualization code]


```r
docs_tf_idf %&gt;%
  mutate(word = reorder_within(word, tf_idf, doc_id)) %&gt;% 
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  facet_wrap(~doc_id, scales = "free_y") +
  coord_flip()  +
  scale_x_reordered() +
  theme_minimal()
```

]


.panel[.panel-name[Visualization]

&lt;img src="figs/unnamed-chunk-63-1.png" width="936" /&gt;




]

]







---


## Calculate tf-idf of Trump tweets

.font70[

When it comes to Trump tweets, we don't have *obvious* documents. But we can easily consider years as *pseudo*-documents with the fair assumption that different words are important per year.

]

.panelset[

.panel[.panel-name[tf-idf]


```r
trump_tf_idf &lt;- trump_words %&gt;%
  filter(date_year %in% 2016:2021) %&gt;%
  count(date_year, word, sort = T) %&gt;%
  bind_tf_idf(word, date_year, n)
```

]


.panel[.panel-name[Top scores per year]


```r
top10_tfidf_per_year &lt;- trump_tf_idf %&gt;%
  arrange(-tf_idf) %&gt;%
  group_by(date_year) %&gt;%
  slice(1:10) %&gt;%
  ungroup()
```
]

.panel[.panel-name[Visualization code]


```r
top10_tfidf_per_year %&gt;% 
  mutate(word = reorder_within(word, tf_idf, date_year)) %&gt;%
  ggplot(aes(word, tf_idf, fill = date_year)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~date_year, scales = "free") +
  scale_x_reordered() +
  coord_flip() +
  theme_minimal()
```
]

.panel[.panel-name[Visualization]

&lt;img src="figs/unnamed-chunk-67-1.png" width="936" /&gt;
]

]



---

class: center, middle, inverse

# Stemming (&amp; lemmatization)

&lt;center&gt;


&lt;img src="https://media1.giphy.com/media/TfG3MXcJizx35mVGH3/200.gif" style="width: 70%"/&gt;

&lt;/center&gt;

---

## Stemming

.font80[
&gt; Stemming (and lemmatization) are two common techniques used to reduce the number of words by transforming it into some base form. 

Stemming chops off the ends of words to remove the suffix. This often results in non-words, or words that are not in the dictionary.
]




```r
create_words &lt;- c("create", "created", "creation", "creating", "creationism") 

SnowballC::wordStem(create_words)
```

```
#&gt; [1] "creat"    "creat"    "creation" "creat"    "creation"
```

.font80[
Notice, how `creation` and `creationism` both become `creation`, two very different meanings, so one should be careful about doing this if you study, for example, creationism.
]



---

## Lemmatization

&gt; Lemmatization involves changing a word to its base dictionary form. 

.font80[

For example: transforming "was" to "be", "running" to "run".

Dictionaries are available for this but they are very crude methods to lemmatize.
]



```r
some_runs &lt;- c("run", "ran", "running")
textstem::lemmatize_words(some_runs)
```

```
#&gt; [1] "run" "run" "run"
```

.font80[
Typcally this requires some knowledge about the words in text (whether it is a noun, verb etc.).

We will apply this on Day 2 when we do **part-of-speech tagging** to find the lemma of a word.
]




---

class: center, middle, inverse

# Dictionary-based methods



---

## Dictionary-based methods

&gt; Dictionary involve looking up words in a pre-defined dictionary and then tagging them according to the concept of interest. 

The most typical implementation of a dictionary method is **sentiment analysis**. In this case, words in a sentence are tagged according to their positive or negative sentiments. 

However, other implementations of dictionary methods exist, for example [moral foundations](https://osf.io/ezn37/).

In this workshop we will only be looking at sentiment analysis, but know that other dictionaries exist!

---

### Validation of *"off-the-shelf"* dictionaries (van Attefeld et al. 2021)

![](images/dictionary_validation.png)

---

## sentiment analysis with `tidytext`

For basic sentiment analysis, we return to the `tidytext` R Package.

The sentiment dictionaries are offered:

* `afinn` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
* `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
* `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also emotions like joy, anger, sadness, and more 

We are going to use the `bing` lexicon categorizes words in a binary fashion into positive and negative categories.

 


---

## sentiment analysis with `bing`

Let's take a look at some of the sentiment tags provided by `bing`:


```r
get_sentiments("bing") %&gt;%
  slice(3, 7, 948, 928, 907)
```

```
#&gt; # A tibble: 5 x 2
#&gt;   word        sentiment
#&gt;   &lt;chr&gt;       &lt;chr&gt;    
#&gt; 1 abolish     negative 
#&gt; 2 abomination negative 
#&gt; 3 compliment  positive 
#&gt; 4 compassion  positive 
#&gt; 5 colorful    positive
```


---


## sentiment analysis with `bing`

We contact the sentiment analysis by joining the sentiment tags into the words used by Trump. `inner_join()` in this case only keeps words that do have a sentiment attached.

For this to work, make sure that your token variable which contains the words is called `word`!


```r
trump_tweets_bing &lt;- trump_tweets %&gt;%
  unnest_tokens(word, text) %&gt;%
  inner_join(get_sentiments("bing"))
```


---


## sentiment analysis with `bing`

.pull-left[

.font80[

Top occuring words with positive sentiment:

]



```r
trump_tweets_bing %&gt;%
  filter(sentiment == "positive") %&gt;% 
  count(word, sort = T) %&gt;% 
  slice(1:5) %&gt;% 
  kable()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; word &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; great &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7672 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; trump &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6573 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; thank &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3609 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; like &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2059 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; good &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1878 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.pull-right[
.font80[
Looks like "trump" is a word that has a positive label. 

*Uh oh*, that is going to be a problem since Trump talks a lot about himself. Ideally, we would have to remove this word (or rename it) as to not trick our sentiment analysis.

But this is one of the typical issues in unigram based sentiment analysis. So we will keep it for now with this caveat in mind.
]
]



---


## sentiment analysis with `bing`

.font80[To get a good understanding of how often positive vs. negative words are used by Trump we can calculate the relative frequency, i.e. the percentage of negative and positive words per year.]


.panelset[

.panel[.panel-name[Visualization code]


```r
trump_tweets_bing %&gt;%
  count(date_year, sentiment, sort = T) %&gt;%
  group_by(date_year) %&gt;%
  mutate(perc = n/sum(n)*100) %&gt;%
  ggplot(aes(date_year, perc, fill = sentiment)) +
  geom_area() +
  theme_minimal()
```
]


.panel[.panel-name[Visualization]

.pull-left[
&lt;img src="figs/unnamed-chunk-74-1.png" width="522.144" /&gt;

]

.pull-right[

.font80[Looks like since his presidency started (2016), Trump became more negative!]

.font60[(or mentioned himself less? less remember that bias!)]
]



]
]


---

## Augmented dictionary methods (`sentimentr`)


Augmented dictionary methods are used to improve the performance of unigram (i.e. one word) dictionaries. These methods are used to add context to words, and modify the strength and directionality of information.

&lt;p style="font-size:1px"&gt;&lt;/p&gt; | &lt;p style="font-size:1px"&gt;&lt;/p&gt;
-------------------|------------------
 &lt;img src="https://github.com/trinker/sentimentr/raw/master/tools/sentimentr_logo/r_sentimentr.png" width = "150"&gt; | &lt;p style="font-size:20px"&gt;"`sentimentr` is an augmented dictionary that takes into account valence shifters and (de-)amplifiers designed to quickly calculate text polarity sentiment in the English language at the sentence level."&lt;/p&gt;
 

```r
library(sentimentr)
```
 
 
---

## Augmented dictionary methods (`sentimentr`)

So what does [sentimentr](https://github.com/trinker/sentimentr) do that other packages don't and why does it matter?

sentimentr attempts to take into account 

+ **valence shifters** (i.e., negators)
  + A negator flips the sign of a polarized word 
    + "I do *not* like it."
+ **amplifiers** (intensifiers)
  + "I *really* like it."
+ **de-amplifiers** (downtoners)
  + "I *hardly* like it."
+ **adversative conjunctions** (conjunction that overrides initial meaning)
  + "I like it but it's *not* worth it."
 


---


## Augmented dictionary methods (`sentimentr`)

Let's calculate the average sentiment of our "nuclear" Trump tweet.


```r
library(magrittr)

trump_tweet %&gt;%
  get_sentences() %$%
  sentiment_by(text, id) %&gt;%
  kable()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; word_count &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ave_sentiment &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1165918301932916736 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3376007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.362493 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

As one would expect, it is classified as negative (below zero). Good.

---


## Augmented dictionary methods (`sentimentr`)




Let's calculate the sentiment for all Trump tweets:


```r
trump_tweets_sentimentr &lt;- trump_tweets %&gt;%
    get_sentences() %$%
    sentiment_by(text, id) %&gt;% 
    left_join(trump_tweets) 
```




---

## Augmented dictionary methods (`sentimentr`)


Most negative tweets:


```r
trump_tweets_sentimentr %&gt;%
  arrange(ave_sentiment) %&gt;%
  head(5) %&gt;%
  pull(text)
```

```
#&gt; [1] "RT @GeraldoRivera: This is just the most egregious example of the rampant unfairness that has tainted this partisan witch-hunt from the beg..."                                                                   
#&gt; [2] "@Samco42  No, but I hate incompetence."                                                                                                                                                                           
#&gt; [3] "“Democrats repeatedly claimed impeachment was an urgent matter, but now Nancy Pelosi uses stall tactics to obstruct the Senate.” @replouiegohmert @HeyTammyBruce @FoxNews  It’s all part of the Impeachment Hoax!"
#&gt; [4] "The debate was pretty even but I thought Mitt should have been much more aggressive on Obama's failed foreign policy-- and I mean much more."                                                                     
#&gt; [5] "\"Captain Khan, killed 12 years ago, was a hero, but this is about RADICAL ISLAMIC TERROR and the weakness of our \"\"leaders\"\" to eradicate it!\""
```

---

## Augmented dictionary methods (`sentimentr`)

Most positive tweets:


```r
trump_tweets_sentimentr %&gt;%
  arrange(-ave_sentiment) %&gt;%
  head(5) %&gt;%
  pull(text)
```

```
#&gt; [1] "I am truly honored and grateful for receiving SO much support from our American heroes...https://t.co/S9bvbysiOr https://t.co/JJQncd3zhf"          
#&gt; [2] "RT @AmericanEsther: @RandPaul @realDonaldTrump Thank the Lord most High!&lt;U+0001F64F&gt;&lt;U+0001F3FB&gt;\nGod bless the peacemakers @realDonaldTrump&lt;U+0001F64F&gt;&lt;U+0001F3FB&gt;\nGod bless our brave…"
#&gt; [3] "I hate to say it, but the Republican Convention was far more interesting (with a much more beautiful set) than the Democratic Convention!"         
#&gt; [4] "RT @realDonaldTrump: My supporters are the smartest, strongest, most hard working and most loyal that we have seen in our countries history..."    
#&gt; [5] "Thanks to everyone for your kind birthday wishes -- very nice!"
```


---

#### Sentiment over time

Let's take a look at how sentiment changes over time.

.panelset[
.panel[.panel-name[Visualization code]



```r
trump_tweets_sentimentr %&gt;%
  ggplot(aes(date, ave_sentiment)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  theme_minimal()
```

]


.panel[.panel-name[Visualization]

.pull-left[
&lt;img src="figs/unnamed-chunk-82-1.png" width="522.144" /&gt;
]



.pull-right[

Looks like sentiment overall seems to slightly decrease. But there is a lot of noise between positive and negative tweets by Trump.

]

]


]


---

### Sentiment by keywords

Let's check how the sentiments differ by different kinds of keywords:

Republicans, Democrats, Islam/Muslims and Christians



```r
mentions_dat &lt;- trump_tweets_sentimentr %&gt;%
  mutate(lower_text = str_to_lower(text)) %&gt;%
  mutate(republican = str_detect(lower_text, ".epublican|gop"),
         democrat = str_detect(lower_text, ".emocrat"),
         islam = str_detect(lower_text, "islam|muslim"),
         christian = str_detect(lower_text, "christian")) 
```

---


### Sentiment by keywords

.pull-left[


```r
mentions_dat %&gt;%
  group_by(republican, democrat, 
           islam, christian) %&gt;%
  summarize(ave_sentiment = mean(ave_sentiment)) %&gt;%
  ungroup() %&gt;%
  kable()
```

]

.pull-right[
&lt;table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; republican &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; democrat &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; islam &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; christian &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ave_sentiment &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0808180 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0582011 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.0274643 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1685149 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.0702306 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1235272 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0679853 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0080690 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FALSE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1645306 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

&lt;br&gt;

.font80[
We can see that the sentiment is positive when mentioning Christians, or Republicans. We see a negative sentiment when mentioning Democrats or Islam/Muslims. No suprises here!
]


---


# That's it for Day 1


.pull-left[

![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)
![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)

![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)
![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)



]

.pull-right[

![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)
![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)

![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)
![](https://media.tenor.com/images/abf1057f2d6774fa38997e8987f18949/tenor.gif)



]





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%<br>",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script>
  (function() {
    var divHTML = document.querySelectorAll(".details-open");
    divHTML.forEach(function (el) {
      var preNodes = el.getElementsByTagName("pre");
      var outputNode = preNodes[1];
      outputNode.outerHTML = "<details open class='output'><summary>Run</summary>" + outputNode.outerHTML + "</details>";
    })
  })();
(function() {
  var divHTML = document.querySelectorAll(".details");
  divHTML.forEach(function (el) {
    var preNodes = el.getElementsByTagName("pre");
    var outputNode = preNodes[1];
    outputNode.outerHTML = "<details class='output'><summary>Run</summary>" + outputNode.outerHTML + "</details>";
  })
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
