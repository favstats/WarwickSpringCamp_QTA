---
title: "Quantitative Text Analysis in R"
subtitle: "A gentle hands-on introduction"
output: 
  xaringan::moon_reader:
    css:
      - default
      - css/styles.css
      - css/fonts.css
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%<br>"
      ratio: "16:9"
    includes:
      after_body: "collapseoutput.js"
---


layout: true



<style>

 .tab { margin-left: 40px; }

.onehundredtwenty {
  font-size: 120%;
   }

<style>
.ninety {
  font-size: 90%;
   }

.eightyfive {
  font-size: 85%;
   }
   
.eighty {
  font-size: 80%;
   }
   
.seventyfive {
  font-size: 75%;
   }
   
.seventy {
  font-size: 70%;
   }
   
.fifty {
  font-size: 50%;
   }
   
.forty {
  font-size: 40%;
   }
</style>


```{r meta, echo=FALSE, warning=F, message=F}

library(tidymodels)
library(metathis)
meta() %>%
  meta_general(
    description = "Quantitative Text Analysis in R",
    generator = "xaringan and remark.js"
  ) %>%
  meta_name("github-repo" = "favstats/xxx") %>%
  meta_social(
    title = "Quantitative Text Analysis in R",
    url = "https://www.favstats.eu",
    og_type = "website",
    og_author = "Fabio Votta",
    twitter_card_type = "summary_large_image",
    twitter_creator = "@favstats"
  )
```



```{r setup, include=FALSE}
# dateWritten <- format(as.Date('2020-05-04'), format="%B %d %Y")
workshop_day <- format(as.Date("2022-06-29"), format="%B %d %Y")
pacman::p_load(xaringanthemer)

options(
    htmltools.dir.version = FALSE,
    knitr.table.format = "html",
    knitr.kable.NA = ""
)
knitr::opts_chunk$set(
    warning = FALSE,
    message = FALSE,
    fig.path = "figs/",
    fig.width = 7.252,
    fig.height = 4,
    comment = "#>",
    fig.retina = 3 # Better figure resolution
)



# Enables the ability to show all slides in a tile overview by pressing "o"
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::use_clipboard()
# xaringanExtra::use_share_again()
# xaringanExtra::style_share_again(share_buttons = "all")
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,
  mute_unhighlighted_code = FALSE
)
# xaringanExtra::use_webcam()


knitr::opts_chunk$set(warning = F, message = F) # Whether to display errors
```



---
name: title-slide
class: title-slide, center, middle


<div class="my-logo-right"></div> 

<br>

# .font150[.fancy[`r rmarkdown::metadata$title`]] 

### .font120[.fancy[`r rmarkdown::metadata$subtitle`]]

*Spring Camp University of Warwick*

Instructor: Fabio Votta

[`r fa(name = "twitter", fill = "white")` @favstats](http://twitter.com/favstats)<br>
[`r fa(name = "github", fill = "white")` @favstats](http://github.com/favstats)<br>
[`r fa(name = "globe", fill = "white")` favstats.eu](https://www.favstats.eu)


29th June 2022 (Day 2)

.fifty[Link to slides: [favstats.github.io/WarwickSpringCamp_QTA/slides/day2/](https://favstats.github.io/WarwickSpringCamp_QTA/slides/day2/)]

---

class: center, middle

# Part-of-speech tagging

<center>

<img src="https://i2.wp.com/www.bnosac.be/images/bnosac/blog/depenceny-parsing-example3.png?w=584">

  
</center>

---


## Part-of-speech tagging

> Part-of-speech tagging is the process of assigning a syntactic tag to each word in a sentence. 

For example, in the sentence: 

`"The dog chased the cat."`

1. "dog" and "cat" would be assigned the tag "noun"

2. "chased" would be assigned the tag "verb." 

The most common parts of speech are nouns, verbs, adjectives, adverbs, and pronouns, but there are many more!

---


## NLP with `udpipe`


> "`udpipe` provides quick and simple annnotations giving rich output: tokenization, part-of-speech-tagging, lemmatization and dependency parsing with multi-language support. From raw text to parsed output for more than 50 languages."

```{r}
library(udpipe)
```


---

### NLP with `udpipe`

.font70[
*The* following table contains the so-called **universal part-of-speech tags** (upos). 

]

.font40[

| Universal POS tags | Meaning                   | Definition                                                                                                                                                                                                                      | Example                                                          |
|--------------------|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|
| ADJ                | adjective                 | Adjectives are words that typically modify nouns and specify their properties or attributes:                                                                                                                                    | The car is **green**.                                            |
| ADP                | adposition                | In many languages, adpositions can take the form of fixed multiword expressions,                                                                                                                                                | **in** spite **of**, because **of**, thanks **to**               |
| ADV                | adverb                    | Adverbs are words that typically modify verbs for such categories as time, place, direction or manner.                                                                                                                          | He ate **slowly**.                                               |
| AUX                | auxiliary                 | An auxiliary is a function word that accompanies the lexical verb of a verb phrase and expresses<br>grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect,<br>voice or evidentiality. | Tense auxiliaries: **has** (done), **is** (doing), **will** (do) |
| CCONJ              | coordinating conjunction  | A coordinating conjunction is a word that links words or larger constituents without syntactically<br>subordinating one to the other and expresses a semantic relationship between them.                                           | **and**, **or**, **but**                                         |
| DET                | determiner                | Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context.                                                                                                                | **the**, **this**, **that**, **which**                           |
| INTJ               | interjection              | An interjection is a word that is used most often as an exclamation or part of an exclamation.                                                                                                                                  | **psst**, **ouch**                                               |
| NOUN               | noun                      | Nouns are a part of speech typically denoting a person, place, thing, animal or idea.                                                                                                                                           | The **cat** is in the **hat**.                                   |
| NUM                | numeral                   | A numeral is a word, functioning most typically as a determiner, adjective or pronoun, that expresses<br>a number and a relation to the number, such as quantity, sequence, frequency or fraction.                                 | **0**, **1**, **2**, **one**, **two**, **three**                 |
| PART               | particle                  | Particles are function words that must be associated with another word or phrase to impart meaning<br>and that do not satisfy definitions of other universal parts of speech                                                       | Possessive marker: [en] â€˜s                                       |
| PRON               | pronoun                   | Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable<br>from the linguistic or extralinguistic context.                                                                                      | personal pronouns: I, you, he, she, it, we, they                 |
| PROPN              | proper noun               | A proper noun is a noun (or nominal content word) that is the name (or part of the name)<br>of a specific individual, place, or object.                                                                                            | **London**, **NATO**, **Mary Sue**                               |
| PUNCT              | punctuation               | Punctuation marks are non-alphabetical characters and character groups used in many languages<br>to delimit linguistic units in printed text.                                                                                      | **,**, **.**, **(**, **:**                                       |
| VERB               | verb                      | A verb is a member of the syntactic class of words that typically signal events and actions                                                                                                                                     | He **runs**.                                                     |



]

---

### One function to rule them all: `udpipe()`

`udpipe()` is the main work horse of the `udpipe` package. With it you can perform

+ tokenization
+ lemmatization
+ part-of-speech tagging
+ dependency parsing

**all in one!**

>  On dependency parsing:  dependency parsing is a type of syntactic parsing that identifies the dependencies between words in a sentence. Dependency parsers typically use a set of rules to find these dependencies, and these rules can vary depending on the language being parsed.


---

#### Trump tweet example


Let's apply `udpipe()` on the "nuclear" Trump tweet from yesterday and see what it can do for us!

```{r}
## read in Trump tweets
trump_tweets <- readr::read_csv("https://raw.githubusercontent.com/favstats/WarwickSpringCamp_QTA/main/docs/slides/day1/data/trump_tweets.csv")


## nuclear tweet
trump_tweet <- trump_tweets[trump_tweets$id == 1165918301932916736,]

trump_tweet$text
```


---

#### Trump tweet example

.pull-left[

`udpipe()` expects a data.frame with two variables:

1. `doc_id`, a unique identifier for your document
2. `text`, the text you are trying to parse



```{r, eval = F}
trump_tweet_ud <- trump_tweet %>%
  mutate(doc_id = id)

pos_tags_trump <- udpipe(trump_tweet_ud, "english")

pos_tags_trump %>%
  select(token, upos, lemma) %>% kable()
```

]

.pull-right[



```{r, echo = F}
trump_tweet_ud <- trump_tweet %>%
  mutate(doc_id = id)

pos_tags_trump <- udpipe(trump_tweet_ud, "english")

pos_tags_trump %>%
  select(token, upos, lemma, dep_rel) %>% 
  slice(1:22) %>%
  kable() %>% kableExtra::kable_styling(font_size = 10) 
```


]


---

### Visualizing the nice tidy data frame that `udpipe()` puts out

.font50[
We can visualize all the output from `udpipe()` using the function (taken from [here](https://www.r-bloggers.com/2019/07/dependency-parsing-with-udpipe/#:~:text=Dependency%20parsing%20is%20an%20NLP,you%20further%20details%20about%20it.)).
]



![](https://pbs.twimg.com/media/FWAb-SFX0AAzW4n?format=jpg&name=large)

---

### Keyword extraction

.font80[


Now we have tags for our text. Brilliant. What can we do with this? 

Similar as before, we are interested in what's going on in our text. 

Part-of-speech tagging can enable us to find more meaningful keywords that tell us something about the texts we are investigating.

`udpipe()` offers various methods for keyword extractions:


1.   Find keywords by doing **parts of speech tagging in order to identify nouns**
2. Find keywords based on **collocations and co-occurrences**
3. Find keywords based on algorithms
  * **RAKE** (rapid automatic keyword extraction)
4. Find keywords by looking for **phrases** (e.g. noun phrases)
5. Find keywords based on results of **dependency parsing** (getting the subject of the text)


]


---

#### Most frequent nouns/verbs etc.

Let's now run part-of-speech tagging to find the most common nouns and verbs that Trump uses.

```{r, eval = F}
# This takes a couple of minutes

trump_tweets_uds <- trump_tweets %>%
  filter(date_year %in% 2016:2021) %>%
  mutate(doc_id = id)  %>%
  mutate(doc_id = as.character(doc_id))

pos_tags_trump_all <- udpipe(trump_tweets_uds, "english")  %>%
  left_join(trump_tweet_ud)
```

```{r, echo = F}
pos_tags_trump_all <- readRDS(url("https://raw.githubusercontent.com/favstats/WarwickSpringCamp_QTA/main/docs/slides/day2/data/pos_tags_trump_all.rds"))

```


---

#### Most frequent nouns


```{r}
pos_tags_trump_all %>%
  filter(upos == "NOUN") %>%
  count(token, sort = T) %>%
  head(10)  %>% 
  mutate(word_n = paste0(token, ": ", n)) %>% 
  pull(word_n)
```


---

#### Most frequent verbs



```{r}
pos_tags_trump_all %>%
  filter(upos == "VERB") %>%
  count(token, sort = T) %>%
  head(10)  %>% 
  mutate(word_n = paste0(token, ": ", n)) %>% 
  pull(word_n)
```

---

#### RAKE (rapid automatic keyword extraction)

> RAKE is a basic algorithm which tries to identify keywords in text. Keywords are defined as a sequence of words following one another. 

Frequency of occurence plays a role as well, as well as frequency of co-occurences with other words. Word combinatiosn with higher values are considered to be more frequent and unique.

If you want to know more about RAKE [see here](https://www.analyticsvidhya.com/blog/2021/10/rapid-keyword-extraction-rake-algorithm-in-natural-language-processing/).


---

#### RAKE (rapid automatic keyword extraction)

.panelset[
.panel[.panel-name[Extract RAKE]


```{r}
## Using RAKE
rake_keys <- keywords_rake(
  x = pos_tags_trump_all %>% mutate(lemma = stringr::str_to_lower(lemma)), 
  term = "lemma", group = "doc_id", 
  relevant = pos_tags_trump_all$upos %in% c("NOUN", "VERB", "ADJ")
  )
                       
```

]


.panel[.panel-name[Visualization code]

```{r, eval = F}

library(ggplot2)


#stats
rake_keys %>% 
  filter(freq > 5) %>%
  arrange(-rake) %>%
  mutate(keyword = forcats::fct_reorder(keyword, rake)) %>%
  slice(1:20) %>%
  ggplot(aes(keyword, rake)) +
  geom_col() +
  coord_flip() +
  theme_minimal()
```

]


.panel[.panel-name[Visualization]

```{r, echo = F}
#stats
rake_keys %>% 
  filter(freq > 5) %>%
  arrange(-rake) %>%
  mutate(keyword = forcats::fct_reorder(keyword, rake)) %>%
  slice(1:20) %>%
  ggplot(aes(keyword, rake)) +
  geom_col() +
  coord_flip() 
```

]

]

---

#### (Simple) Noun Phrases



Next option is to extract (simple) noun phrases. What are they?

Noun phrases are groups of words that function like nouns.

Some examples:

**All the children** were eating.

She bought herself **a beautiful dark dress**.

Dad baked **a tasty chocolate cake**.



---

#### (Simple) Noun Phrases

.font80[

How does this work? Parts of Speech tags are recoded to one of the following one-letters: 

> A: adjective, C: coordinating conjuction, D: determiner, M: modifier of verb, N: noun or proper noun, P: pre/postposition. 

Next you can define a regular expression to indicate a sequence of parts of speech tags which you want to extract from the text.

As regex we can express a (simple) noun phrase as this:

> `(A|N)*N(P+D*(A|N)*N)*`

For more info on noun phrases [see here](https://universaldependencies.org/workgroups/newdoc/simple_noun_phrases.html).

]


---


#### (Simple) Noun Phrases

```{r}
## Simple noun phrases (a adjective+noun, pre/postposition, optional determiner and another adjective+noun)
pos_tags_trump_all$phrase_tag <- as_phrasemachine(pos_tags_trump_all$upos, type = "upos")

keyw_nounphrases <- keywords_phrases(pos_tags_trump_all$phrase_tag, term = pos_tags_trump_all$token, 
                                     pattern = "(A|N)*N(P+D*(A|N)*N)*", is_regex = TRUE, 
                                     detailed = T)


```


---


#### Most common (simple) noun phrases

```{r}
keyw_nounphrases %>%
  filter(ngram >= 3) %>%
  count(keyword, sort = T) %>%
  head(20)  %>% 
  mutate(keyword_n = paste0(keyword, ": ", n)) %>% 
  pull(keyword_n)
```

---

#### Dependency Parsing

For this exercise we are going to take the words which have as dependency relation "*nsubj*" indicating the nominal subject and we are adding to that the adjective which is changing the nominal subject.


```{r}
stats <- merge(pos_tags_trump_all, pos_tags_trump_all, 
           by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
           by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
           all.x = TRUE, all.y = FALSE, 
           suffixes = c("", "_parent"), sort = FALSE) 

stats <- subset(stats, dep_rel %in% c("nsubj") & upos %in% c("NOUN", "PROPN") & upos_parent %in% c("ADJ"))

stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")


```

---


#### Dependency Parsing

```{r}
stats %>%
  count(term, sort = T) %>%
  head(20)   %>% 
  mutate(term_n = paste0(term, ": ", n)) %>% 
  pull(term_n)
```

---

class: center, middle, inverse

# Machine Learning

<center>

<img src="https://i.gifer.com/fxvV.gif">

  
</center>

---

## Machine Learning

> Machine learning, broadly defined, are a range computational methods that can "learn" by discovering patterns in data.

There are two (for us relevant) methods how machines can learn:

1. *Unsupervised* machine learning (clustering)
2. *Supervised* machine learning (classification)



---

## Machine Learning

> Machine learning, broadly defined, are a range computational methods that can "learn" by discovering patterns in data.

**Unsupervised machine learning (clustering)**

In *unsupervised learning* we let an algorithm figure out quantities of interest, solely by supplying it with the data.

Typically, this happens by identifiyng so-called *clusters* that are more similar to each other in some way, compared to other clusters.


---

## Machine Learning

> Machine learning, broadly defined, are a range computational methods that can "learn" by discovering patterns in data.


**Supervised machine learning (classification)**

In *supervised learning* we provide a machine learning model with some data (*training data*) from which it can learn patterns.

For example, we may have some annotated tweets that tell us whether they are positive or negative. We can leverage that data to let a machine learning model learn the patterns of text that are positive vs. negative.

---

## Machine Learning

Most important thing to remember:

+ *Unsupervised* machine learning 

<p class="tab">&#8594; learns patterns by itself</p>



+ *Supervised* machine learning (classification)



<p class="tab">&#8594; learns patterns based on provided (human) annotations</p>




---

class: center, middle, inverse

# Topic Models

an .fancy[unsupervised machine learning] method

---


## Topic Models

.font80[
> Topic models can be used to identify "topics" in (large corpora of) text. 


You may have noticed that I put "topics" in quotation marks. That is because the output of a topic model is not what one would *intuitively* understand as topic.

Here is an example output:
]



![](images/topics_output.png)

---



## Topic Models

We are going to take a look at Latent Dirichlet Allocation (LDA) in particular (a common implementation of topic modelling)

An LDA has two model assumptions:

+ every document can be represented as a distribution of topics - $\theta$

+ every topic can be represented as distribution of terms - $\beta$


---

## Document-term matrix

Most topic models take data as a so-called "*document-term-matrix* (dtm). A dtm is a matrix (with rows and columns) that counts the number of times terms occur in specific documents.

This is what it looks like:



---

## Topic Models

We could estimate our topic models using all the words within the document but:

1. Many words will not be good indicators of a topic
2. It will take a long time

So in the interest of time, we will **only keep lemmatized nouns** (as identified by POS tagging).

A similar approach has been taken by Jacobi et al., 2016, and Lind et al., 2021.

---

```{r, eval = F}
reddit_dtm <- sotu_ud %>%
  filter(upos == "NOUN") %>% 
  count(doc_id, word, sort = T) %>%
  filter(n > 50) %>% 
  cast_dtm(submission_id, word, n)
```


---

```{r}
# install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")


quanteda.corpora:: %>% 
  tidy() %>% 
  pull(text) %>% 
  .[2] %>% 
  str_split("____") %>% 
  unlist() %>% 
  .[2]
```

```{r}
debate_transcripts <- readr::read_csv("https://raw.githubusercontent.com/favstats/WarwickSpringCamp_QTA/main/docs/slides/day1/data/debate_transcripts.csv") %>% 
  filter(candidate == 1) %>% 
  rename(doc_id = id) %>% 
  mutate(text = str_replace_all(text, "Bill Clinton", "Clinton"))

library(udpipe)

debates_ud <- udpipe(debate_transcripts, "english", parallel.cores = 4)

saveRDS(debates_ud, file = "data/debates_ud.rds")

library(tidyverse)

debates_ud_nouns <- debates_ud %>% 
  filter(upos %in% c("NOUN", "PROPN"))   %>%
  # filter(upos != "PUNCT") %>% 
  mutate(lemma = str_to_lower(lemma)) %>% 
  # anti_join(stop_words %>% rename(lemma = word)) %>% 
  # left_join(yo %>% select(doc_id, speech_doc_id) %>% mutate(doc_id = as.character(doc_id))) %>% 
  count(doc_id, lemma, sort = T) %>%
  # rename(doc_id = speech_doc_id) %>% 
  filter(n >= 5) %>% 
  # filter(str_detect(lemma, "Bernie"))
  # count(lemma, sort = T) %>%
  filter(!(lemma %in% c("donald", "obama", "bush",
                        "joe", "john", "hillary",
                        "reagan", "george", "barack",
                        "bernie", "sanders", "al",
                        "romney", "kennedy", "mccain",
                        "biden", "ronald", "mccain",
                        "carter", "gore", "kerry",
                        "bill", "elizabeth", "klobuchar",
                        "bloomberg", "mike", "pence",
                        "clinton", "laughter", "applause",
                        "crosstalk", "inaudible")))
  
library(tidytext)
debates_dtm <- debates_ud_nouns %>% 
  cast_dtm(doc_id, lemma, n)

debate_transcripts %>% 
  count(date)
  filter(str_detect(text, "\\["))
```

```{r, eval = F}
require(topicmodels)

# number of topics
# K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(debates_dtm, k =  20)
```
```{r, eval = F}
terms(topicModel, 10)
```
```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)

# for every document we have a probaility distribution of its contained topics
theta <- tmResult$topics 
# dim(theta)               # nDocs(DTM) distributions over K topics

# tidy(topicModel)
jj <- terms(topicModel, 5)

topic_names <- jj %>% 
  as.data.frame() %>% 
  map(~paste0(.x, " ", collapse = "")) %>% 
  unlist() %>% 
  str_trim()

ww <- theta %>% 
  as.data.frame() %>%
  set_names(topic_names) %>% 
  rownames_to_column("doc_id") %>% 
  left_join(debate_transcripts %>% mutate(doc_id = as.character(doc_id))) %>%
  # left_join(yo %>% mutate(doc_id = as.character(speech_doc_id))) %>% 
  mutate(decade = lubridate::floor_date(date, "4 years")) 

library(Polychrome)

data(alphabet)


ww %>% 
  group_by(decade) %>% 
  summarize_at(vars(2:21), mean) %>% 
  gather(key, values, -decade) %>% 
  ggplot(aes(x=decade, y=values, fill=key)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet[1:20], "FF"), name = "decade") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r, eval = F}
yo <- read_csv2("https://raw.githubusercontent.com/tm4ss/tm4ss.github.io/master/data/sotu_paragraphs.csv")

yo2 <- read_csv2("https://raw.githubusercontent.com/tm4ss/tm4ss.github.io/master/data/sotu.csv")

yo %>% count(date)

yo %>% 
  sample_n(10) %>% 
  pull(text)
```
```{r, eval = F}
library(udpipe)

# sotu_ud <- udpipe(yo, "english", parallel.cores = 4)

# saveRDS(sotu_ud, file = "data/sotu_ud.rds")

sotu_ud <- readRDS("data/sotu_ud.rds")

library(tidytext)

library(quanteda)

dfm <- dfm_trim(cast_dfm(sotu_ud_nouns, doc_id, lemma, n), 
                min_docfreq = 0.01, 
                max_docfreq = 0.99, 
                docfreq_type = "prop", 
                verbose = TRUE) 

sotu_ud_nouns <- sotu_ud %>% 
  # filter(upos %in% c("NOUN", "PROPN"))   %>%
  # filter(upos != "PUNCT") %>% 
  mutate(lemma = str_to_lower(lemma)) %>% 
  # anti_join(stop_words %>% rename(lemma = word)) %>% 
  # left_join(yo %>% select(doc_id, speech_doc_id) %>% mutate(doc_id = as.character(doc_id))) %>% 
  count(doc_id, lemma, sort = T) #%>%
  # rename(doc_id = speech_doc_id) %>% 
  filter(n >= 5)

library(tidytext)
  

sotu_dtm <- sotu_ud_nouns %>% 
  cast_dtm(doc_id, lemma, n)

sotu_ud %>% count(upos)
```


```{r, eval = F}
require(topicmodels)

# number of topics
# K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(convert(dfm, to = "topicmodels"), k =  20)
```
```{r, eval = F}
terms(topicModel, 10)
```
```{r, eval = F}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)

# for every document we have a probaility distribution of its contained topics
theta <- tmResult$topics 
# dim(theta)               # nDocs(DTM) distributions over K topics

# tidy(topicModel)
jj <- terms(topicModel, 5)

topic_names <- jj %>% 
  as.data.frame() %>% 
  map(~paste0(.x, " ", collapse = "")) %>% 
  unlist() %>% 
  str_trim()

ww <- theta %>% 
  as.data.frame() %>%
  set_names(topic_names) %>% 
  rownames_to_column("doc_id") %>% 
  left_join(yo %>% mutate(doc_id = as.character(doc_id))) %>%
  # left_join(yo %>% mutate(doc_id = as.character(speech_doc_id))) %>% 
  mutate(decade = lubridate::floor_date(date, "10 years")) 

library(Polychrome)

data(alphabet)


ww %>% 
  group_by(decade) %>% 
  summarize_at(vars(2:21), mean) %>% 
  gather(key, values, -decade) %>% 
  ggplot(aes(x=decade, y=values, fill=key)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet[1:20], "FF"), name = "decade") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


  
```




---

## Supervised machine learning

```{r}
library(tidymodels)

```



```{r}

toxicity_dat <- readr::read_csv("https://raw.githubusercontent.com/surge-ai/toxicity/main/toxicity_en.csv")

```



```{r}
library(tidymodels)


tox_split <- initial_split(toxicity_dat %>% mutate(is_toxic = factor(is_toxic, levels= c("Toxic", "Not Toxic"))), strata = is_toxic)

tox_train <- training(tox_split)
tox_test <- testing(tox_split)
```


```{r}
tox_rec <-
  recipe(is_toxic ~ text, data = tox_train)
```


```{r}
library(textrecipes)

tox_rec <- tox_rec %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = 1e3) %>%
  step_tfidf(text)
```

```{r}
tox_wf <- workflow() %>%
  add_recipe(tox_rec)


tox_wf
```

```{r}
library(discrim)
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec
```




```{r}
nb_fit <- tox_wf %>%
  add_model(nb_spec) %>%
  fit(data = tox_train)
```

```{r}
set.seed(234)
tox_folds <- vfold_cv(tox_train)

tox_folds
```



```{r}
nb_wf <- workflow() %>%
  add_recipe(tox_rec) %>%
  add_model(nb_spec)

nb_wf
```

```{r}
nb_rs <- fit_resamples(
  nb_wf,
  tox_folds,
  control = control_resamples(save_pred = TRUE)
)
```


```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
```


```{r}
nb_rs_metrics

```

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = is_toxic, .pred_Toxic) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for US Consumer Finance Complaints",
    subtitle = "Each resample fold is shown in a different color"
  )
```
```{r}
nb_rs_predictions %>%
  recall(is_toxic, .pred_class)
```

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  recall(is_toxic, .pred_class)
```





```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")
```



```{r}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(tox_rec) %>%
  add_model(null_classification) %>%
  fit_resamples(
    tox_folds
  )
```


```{r}
null_rs %>%
  collect_metrics()
```










